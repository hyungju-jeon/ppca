{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Probabilistic Principal Component Analysis Implementation\n",
    "Implementation of Probabilistic PCA (ppca) algorithm using EM algorithm\n",
    "\n",
    "**References**\n",
    "* Tipping, M. E., & Bishop, C. M. (1999). Probabilistic Principal Component Analysis, 611â€“622.\n",
    "\n",
    "***\n",
    "## Probability model\n",
    "Our latent variable model for d-dimensional observation vector $y$ with q-dimentional latent vector $x$ is defined as follows\n",
    "\\begin{align}\n",
    "y &= \\mathbf{W}x + \\mu + \\epsilon \\\\\n",
    "x &\\sim \\mathcal{N} (0,I) \\\\\n",
    "\\epsilon &\\sim \\mathcal{N} (0,\\sigma^2I)\n",
    "\\end{align}\n",
    "\n",
    "where $ \\mathbf{W} $ is $d \\times q$ matrix. \n",
    "\n",
    "Then the conditional distribution of $y$ over $x$ is given by\n",
    "\\begin{equation}\n",
    "p(y|x) = \\mathcal{N} (\\mathbf{W}x+\\mu, \\sigma^2I)\n",
    "\\end{equation}\n",
    "and marginal distribution of $y$\n",
    "\\begin{equation}\n",
    "p(y) = \\mathcal{N} (\\mu, C)\n",
    "\\end{equation}\n",
    "where the observation covariance is specified by \n",
    "\\begin{equation}\n",
    "C = \\mathbf{W}\\mathbf{W}^\\intercal +\\sigma^2I\n",
    "\\end{equation}\n",
    "\n",
    "Finally, the posterior distribution $p(x|y)$ can be computed as follows\n",
    "\\begin{align}\n",
    "p(x|y,\\mathbf{W},\\sigma) & = \\mathcal{N} \\left(\\mathbf{W}^\\intercal C^{-1} \\left(y - \\mu \\right),  I - \\mathbf{W}^\\intercal C^{-1} \\mathbf{W}\\right) \\\\\n",
    "                         & = \\mathcal{N} \\left( M^{-1}\\mathbf{W}^\\intercal \\left(y - \\mu \\right),  \\sigma^2 M^{-1}\\right)\n",
    "\\end{align}\n",
    "where $ M = \\mathbf{W}^\\intercal\\mathbf{W} +\\sigma^2I $.\n",
    "***\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM for PPCA\n",
    "The complete log-likelihood of the model, including both observation $\\mathbf{Y} = (y_1,y_2,\\cdots,y_N)$ and latent variable $\\mathbf{X} = (x_1,x_2,\\cdots,x_N)$, is\n",
    "\\begin{align}\n",
    "\\mathcal{L} & = \\log p(\\mathbf{Y},\\mathbf{Z}|\\mu, \\mathbf{W}, \\sigma^2)  \\\\\n",
    "            & = \\sum_{i=1}^N \\left( \\log p(y_i|x_i) + \\log p(x_i)\\right) \\\\\n",
    "\\end{align}\n",
    "### E-step\n",
    "In the $\\mathbf{E}$-Step, we take the expectation of $\\mathcal{L}$ with respect to the posterior distribution $p(x|y,\\mathbf{W},\\sigma)$\n",
    "\\begin{align}\n",
    "\\mathbf{E}\\left[\\mathcal{L}\\right]  = &-\\frac{nd}{2}\\log2\\pi\\sigma^2 \n",
    "                                       -\\frac{1}{2\\sigma^2}\\sum_{i=1}^N||y_i-\\mu||_2 \n",
    "                                       +\\frac{1}{\\sigma^2}\\sum_{i=1}^N \\mathbf{E}[x_i | y_i]^\\intercal \\mathbf{W}^\\intercal (y_i-\\mu) \n",
    "                                       -\\frac{1}{2\\sigma^2}Tr\\left( \\mathbf{E}[x_i x_i^\\intercal|y_i]\\mathbf{W}^\\intercal \\mathbf{W} \\right)\\\\\n",
    "                                    &-\\frac{nq}{2}\\log2\\pi -\\frac{1}{2}\\sum_{i=1}^N \\mathbf{E}[x_i x_i^\\intercal|y_i]\n",
    "\\end{align}\n",
    "The conditional expectations $\\mathbf{E}[x_i|y_i]$ and $\\mathbf{E}[x_i x_i^\\intercal|y_i]$ can be estimated using current value\n",
    "\\begin{align}\n",
    "\\mathbf{E}[x_i|y_i] & =  M^{-1}\\mathbf{W}^\\intercal \\left(y - \\bar{y} \\right) \\\\\n",
    "\\mathbf{E}[x_i x_i^\\intercal|y_i] & = Var(x_i) + \\mathbf{E}[x_i]\\mathbf{E}[x_i]^\\intercal  \\\\\n",
    "                                  & = \\sigma^2 M^{-1}+ \\mathbf{E}[x_i]\\mathbf{E}[x_i]^\\intercal  \n",
    "\\end{align}\n",
    "\n",
    "### M-step\n",
    "In the $\\mathbf{M}$-Step, we maximize $\\mathbf{E}\\left[\\mathcal{L}\\right]$ with respect to $\\mathbf{W}$ and $\\sigma^2$.\n",
    "First, take the derivative of $\\mathbf{E}\\left[\\mathcal{L}\\right]$ with respect to $\\mathbf{W}$ and set it to 0, to get the updated $\\mathbf{W}_{new}$\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathbf{E}\\left[\\mathcal{L}\\right]}{\\partial \\mathbf{W}}\n",
    "& = \\frac{1}{\\sigma^2}\\sum_{i=1}^N (y_i-\\mu) \\mathbf{E}[x_i|y_i]^\\intercal   \n",
    "    - \\frac{1}{2\\sigma^2}\\sum_{i=1}^N Tr\\left(\\mathbf{W}_{new}\\mathbf{E}[x_i x_i^\\intercal|y_i]\\right)\\\\\n",
    "& = 0 \\\\ \\\\\n",
    "\\mathbf{W}_{new} & = \\left( \\sum_{i=1}^N (y_i-\\mu) \\mathbf{E}[x_i|y_i]^\\intercal \\right) \\left( \\sum_{i=1}^N\\mathbf{E}[x_i x_i^\\intercal|y_i] \\right)^{-1}\n",
    "\\end{align}\n",
    "\n",
    "Next, take the derivative of $\\mathbf{E}\\left[\\mathcal{L}\\right]$ with respect to $\\sigma^2$ and set it to 0, to get the updated $\\sigma^2_{new}$\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\mathbf{E}\\left[\\mathcal{L}\\right]}{\\partial \\sigma^2}\n",
    "& = -\\frac{nd}{2\\sigma^2}\n",
    "    +\\frac{1}{2\\sigma^4}\\sum_{i=1}^N||y_i-\\mu||_2 \n",
    "    + \\frac{1}{\\sigma^4}\\sum_{i=1}^N\\mathbf{E}[x_i | y_i]^\\intercal \\mathbf{W}^\\intercal (y_i-\\mu) \n",
    "    + \\frac{1}{2\\sigma^4}\\sum_{i=1}^NTr\\left(\\mathbf{E}[x_i x_i^\\intercal|y_i]\\mathbf{W}^\\intercal \\mathbf{W}\\right) \\\\\n",
    "& = 0 \\\\ \\\\\n",
    "\\sigma^2_{new} & = \\frac{1}{nd} \\sum_{i=1}^N \\left(||y_i-\\mu||_2 - 2\\mathbf{E}[x_i | y_i]^\\intercal \\mathbf{W}_{new}^\\intercal (y_i-\\mu) + Tr\\left(\\mathbf{E}[x_i x_i^\\intercal|y_i]\\mathbf{W}_{new}^\\intercal \\mathbf{W}_{new}\\right)\\right)\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import sklearn.datasets as ds\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPCA(object):\n",
    "\t\"\"\"\n",
    "    \tEM-based PPCA \n",
    "    \"\"\"\n",
    "\tdef __init__(self, q=2, sigma=1.0, max_iter=50):\n",
    "\t\t# OBSERVED VARIABLE\n",
    "\t\t# y : observed variable (data)\n",
    "\t\tself.y = None\n",
    "\t\t# d : data dimentionality\n",
    "\t\tself.d = 0\n",
    "\t\t# N : number of data \n",
    "\t\tself.N = 0\n",
    "\t\t# mu : data mean (y_bar)\n",
    "\t\tself.mu = None\n",
    "\t\t# Latent VARIABLE\n",
    "\t\t# q : latent variable dimensionality \n",
    "\t\tself.q = q\n",
    "\t\t# sigma : standard deviation of the noise\n",
    "\t\tself.sigma = sigma\n",
    "\t\t# W : d x q matrix relating observed vs. latent variable \n",
    "\t\tself.W = None\n",
    "\t\t# EM properties\n",
    "\t\t# max_iter : maximum iterations\n",
    "\t\tself.max_iter = max_iter\n",
    "\t\t\t\n",
    "\tdef fit(self, data):\n",
    "\t\tself.y = data\n",
    "\t\tself.d = data.shape[0]\n",
    "\t\tself.N = data.shape[1]\n",
    "\t\tself.mu = np.mean(data, axis=0)\n",
    "\t\t# initialize W to small random numbers\n",
    "\t\tprint('Starting EM algorithm')\n",
    "\t\tW = np.random.rand(self.d, self.q)\n",
    "\t\tmu = self.mu\n",
    "\t\tsigma = self.sigma\n",
    "\t\tq = self.q\n",
    "\t\ty = self.y\n",
    "\t\tfor i in range(self.max_iter):\n",
    "\t\t\tprint(f'Running E-step of iter {i}')\n",
    "\t\t\tM = np.transpose(W).dot(W) + sigma * np.eye(q)\n",
    "\t\t\tinvM = np.linalg.inv(M)\n",
    "\t\t\texpZ = invM.dot(np.transpose(W)).dot((y-mu).T)\n",
    "\t\t\texpZZ = sigma*invM + expZ.dot(np.transpose(expZ))\t\t\t\n",
    "\t\t\tprint(f'Running M-step of iter {i}')\n",
    "\t\t\tW = (np.transpose(y-mu).dot(np.transpose(expZ))).dot(np.linalg.inv(expZZ))\n",
    "\t\t\tone =  np.linalg.norm(y-mu)   \n",
    "\t\t\ttwo = 2*np.trace( np.transpose(expZ).dot(np.transpose(W)).dot((y-mu).T) )\n",
    "\t\t\tthree = np.trace(expZZ.dot(np.transpose(W).dot(W)))\n",
    "\t\t\tsigma = one -two + three\n",
    "\t\t\tsigma = (1/(self.N*self.D))*sigma\n",
    "\t\t\tsigma = np.absolute(sigma)\n",
    "\t\tself.W = W\n",
    "\t\tself.sigma = sigma\n",
    "\n",
    "\n",
    "\tdef transform(self, data=None):\n",
    "\t\tif data is None:\n",
    "\t\t\ty = self.y\n",
    "\t\t[W, mu, sigma] = [self.W, self.mu, self.sigma]\n",
    "\t\tM = np.transpose(W).dot(W) + sigma * np.eye(self.q)\n",
    "\t\tinvM = np.linalg.inv(M)\n",
    "\t\tx = invM.dot(np.transpose(W)).dot(y - mu)\n",
    "\t\treturn x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting EM algorithm\n",
      "Running E-step of iter 0\n",
      "Running M-step of iter 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (150,4) and (150,2) not aligned: 4 (dim 1) != 150 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a5f1dd9e5289>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mppca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mppca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miris_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-493deddf9f11>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     43\u001b[0m                         \u001b[0mexpZZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minvM\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexpZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Running M-step of iter {i}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                         \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpZZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m                         \u001b[0mone\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                         \u001b[0mtwo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (150,4) and (150,2) not aligned: 4 (dim 1) != 150 (dim 0)"
     ]
    }
   ],
   "source": [
    "iris = ds.load_iris()\n",
    "iris_y = np.transpose(iris.data)\n",
    "iris_classes = iris.target\n",
    "\n",
    "ppca = PPCA()\n",
    "ppca.fit(iris_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
